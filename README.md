# Enhancing-Large-Language-Model-with-Human-Feedback-via-Reinforcement-Learning

This repository contains code and resources for a project exploring the integration of human feedback into Reinforcement Learning models. The project investigates how incorporating human insights can enhance the learning process and decision-making abilities of AI systems.

## Overview

Reinforcement Learning, a branch of machine learning, involves training algorithms to make decisions based on feedback from the environment. In this project, we delve into the augmentation of RL by integrating human feedback, allowing AI systems to learn from human expertise.

## Features

- **Description of Transformers:** Explain the workings of Transformer including the mathematical foundation and architecture specifics
- **Development of Causal Language Model and Self-Supervised Learning:** Delving into these concepts using the example of GPT
- **Exploration of Markov Decision Processes (MDPs):** Understanding the theoretical foundations of decision-making under uncertainty.
- **Reinforcement Learning Models:** Developing and implementing PPO reinforcement learning model.
- **Integration of Human Feedback:** Investigating methods to incorporate qualitative human insights into RL algorithms.


## Contents

- `notebooks/`: Contains Jupyter Notebook files:
  - `Reward_Modeling.ipynb`: Notebook for implementing RL algorithms.
  - `PPO_Implementation.ipynb`: Notebook for integrating human feedback into RL.
- `reports/`: Contains project report:
  - `project_report.pdf`: Detailed report on the project methodology, findings, and insights.
- `README.md`: Main project overview and instructions.

## References

Code structure is inspired by :

[Quickstart](https://huggingface.co/docs/trl/quickstart), [Reward Modeling](https://huggingface.co/docs/trl/reward_trainer), [PPO Trainer](https://huggingface.co/docs/trl/ppo_trainer)



