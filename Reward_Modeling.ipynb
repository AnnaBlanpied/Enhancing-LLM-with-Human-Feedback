{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  **Reward Modeling**"
      ],
      "metadata": {
        "id": "n5Wt43UchSOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ENBSbmYKuNH8"
      },
      "outputs": [],
      "source": [
        "#!pip install tyro\n",
        "#!pip install transformers\n",
        "#!pip install torch\n",
        "#!pip install datasets\n",
        "#!pip install accelerate\n",
        "#!pip install peft\n",
        "#!pip install trl\n",
        "#!pip install peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary modules and packages"
      ],
      "metadata": {
        "id": "M7RJpjrIjP6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML tasks\n",
        "import torch\n",
        "import tyro\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Data handling and modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from __future__ import annotations\n",
        "from accelerate import Accelerator\n",
        "\n",
        "\n",
        "# TRL library for RL\n",
        "from trl.core import LengthSampler\n",
        "from trl import (\n",
        "    RewardConfig,\n",
        "    RewardTrainer,\n",
        "    is_xpu_available,\n",
        "    AutoModelForCausalLMWithValueHead,\n",
        "    PPOConfig,\n",
        "    PPOTrainer\n",
        ")\n",
        "\n",
        "# Libraries for NLP\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainerCallback,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    GPT2Tokenizer,\n",
        "    pipeline\n",
        ")"
      ],
      "metadata": {
        "id": "M7sdyLG62k8M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Model"
      ],
      "metadata": {
        "id": "b6AeOyBtkJTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "    model_name: str = \"facebook/opt-350m\"\n",
        "    \"\"\"the model name\"\"\"\n",
        "    dataset_name: str = \"Anthropic/hh-rlhf\"\n",
        "    \"\"\"the dataset name\"\"\"\n",
        "    dataset_text_field: str = \"text\"\n",
        "    \"\"\"the text field of the dataset\"\"\"\n",
        "    eval_split: str = \"none\"\n",
        "    \"\"\"the dataset split to evaluate on; default to 'none' (no evaluation)\"\"\"\n",
        "    load_in_8bit: bool = False\n",
        "    \"\"\"load the model in 8 bits precision\"\"\"\n",
        "    load_in_4bit: bool = False\n",
        "    \"\"\"load the model in 4 bits precision\"\"\"\n",
        "    trust_remote_code: bool = True\n",
        "    \"\"\"Enable `trust_remote_code`\"\"\"\n",
        "    reward_config: RewardConfig = field(\n",
        "        default_factory=lambda: RewardConfig(\n",
        "            output_dir=\"output\",\n",
        "            per_device_train_batch_size=8,  #Choosing th batch size as small as possible to be able to train the model\n",
        "            num_train_epochs=1,\n",
        "            gradient_accumulation_steps=16,\n",
        "            gradient_checkpointing=True,\n",
        "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "            learning_rate=1.41e-5,\n",
        "            report_to=\"tensorboard\",\n",
        "            remove_unused_columns=False,\n",
        "            optim=\"adamw_torch\",\n",
        "            logging_steps=500,\n",
        "            evaluation_strategy=\"no\",\n",
        "            max_length=512,\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=ScriptArguments.load_in_8bit, load_in_4bit=ScriptArguments.load_in_4bit)\n",
        "\n",
        "# Step 1: Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'facebook/opt-350m',\n",
        "    num_labels=1,\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Load the dataset and pre-process it\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "train_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:1%]\") #Reducing the size of the training set to be able to train the model\n",
        "\n",
        "\n",
        "# Tokenize chosen/rejected pairs of inputs\n",
        "# Adapt this section to your needs for custom datasets\n",
        "def preprocess_function(examples):\n",
        "    new_examples = {\n",
        "        \"input_ids_chosen\": [],\n",
        "        \"attention_mask_chosen\": [],\n",
        "        \"input_ids_rejected\": [],\n",
        "        \"attention_mask_rejected\": [],\n",
        "    }\n",
        "    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
        "        tokenized_chosen = tokenizer(chosen)\n",
        "        tokenized_rejected = tokenizer(rejected)\n",
        "\n",
        "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
        "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "\n",
        "# Step 3 :Preprocess the dataset and filter out examples that are longer than ScriptArguments.max_length\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "train_dataset = train_dataset.filter(\n",
        "    lambda x: len(x[\"input_ids_chosen\"]) <= 512\n",
        "    and len(x[\"input_ids_rejected\"]) <= 512\n",
        ")\n",
        "\n",
        "\n",
        "# Step 4: Define the LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "            r=16,\n",
        "            lora_alpha=16,\n",
        "            bias=\"none\",\n",
        "            task_type=\"SEQ_CLS\",\n",
        "            modules_to_save=[\"scores\"],\n",
        "        )\n",
        "\n",
        "eval_dataset=None\n",
        "\n",
        "\n",
        "# Step 5: Define the Trainer\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args= RewardConfig(\n",
        "            output_dir=\"output\",\n",
        "            per_device_train_batch_size=8,\n",
        "            num_train_epochs=1,\n",
        "            gradient_accumulation_steps=16,\n",
        "            gradient_checkpointing=True,\n",
        "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "            learning_rate=1.41e-5,\n",
        "            report_to=\"tensorboard\",\n",
        "            remove_unused_columns=False,\n",
        "            optim=\"adamw_torch\",\n",
        "            logging_steps=500,\n",
        "            evaluation_strategy=\"no\",\n",
        "            max_length=512,\n",
        "        ),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=peft_config\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRYkBgmz2oJG",
        "outputId": "d3419447-08a6-4d97-9ee3-716fbde1ecfe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Reward Model"
      ],
      "metadata": {
        "id": "_9rJMO1HkL_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "V6TFCk6W6_9G",
        "outputId": "c5c0a2b9-de36-4a8c-d3c7-9f2628e46d40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 09:10, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=12, training_loss=0.9080644448598226, metrics={'train_runtime': 599.6848, 'train_samples_per_second': 2.605, 'train_steps_per_second': 0.02, 'total_flos': 0.0, 'train_loss': 0.9080644448598226, 'epoch': 0.98})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}