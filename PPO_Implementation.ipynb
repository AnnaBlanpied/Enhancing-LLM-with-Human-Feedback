{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimization with Proximal Policy Optimization**"
      ],
      "metadata": {
        "id": "PfPgO75KlaQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tyro\n",
        "#!pip install transformers\n",
        "#!pip install torch\n",
        "#!pip install datasets\n",
        "#!pip install accelerate\n",
        "#!pip install peft\n",
        "#!pip install trl\n",
        "#!pip install peft"
      ],
      "metadata": {
        "id": "ju-DjSSZ0xLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary modules and packages"
      ],
      "metadata": {
        "id": "I1XcYfUzmAVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML tasks\n",
        "import torch\n",
        "import tyro\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Data handling and modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from __future__ import annotations\n",
        "from accelerate import Accelerator\n",
        "\n",
        "\n",
        "# TRL library for RL\n",
        "from trl.core import LengthSampler\n",
        "from trl import (\n",
        "    RewardConfig,\n",
        "    RewardTrainer,\n",
        "    is_xpu_available,\n",
        "    AutoModelForCausalLMWithValueHead,\n",
        "    PPOConfig,\n",
        "    PPOTrainer\n",
        ")\n",
        "\n",
        "# Libraries for NLP\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainerCallback,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    GPT2Tokenizer,\n",
        "    pipeline\n",
        ")"
      ],
      "metadata": {
        "id": "dc8KXhAx123p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO Simple Implementation :  One query\n"
      ],
      "metadata": {
        "id": "Dj6EWmsYl4PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Load a pretrained model\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
        "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "reward_model = \"lvwerra/distilbert-imdb\"\n",
        "reward_pipe = pipeline(\"sentiment-analysis\", reward_model , device=device)\n",
        "\n",
        "model.cuda()\n",
        "model_ref.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# 2 Initialize trainer\n",
        "\n",
        "ppo_config = {\"batch_size\" : 1}\n",
        "config = PPOConfig(**ppo_config)\n",
        "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "# 3 Encode a query\n",
        "\n",
        "query_txt = \"I love ice\"\n",
        "query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)\n",
        "\n",
        "\n",
        "\n",
        "# 4 Generate model response\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\" : -1,\n",
        "    \"top_k\" : 0.0,\n",
        "    \"top_p\" : 1.0,\n",
        "    \"do_sample\" : True,\n",
        "    \"pad_token_id\" : tokenizer.eos_token_id,\n",
        "    \"max_new_tokens\" : 20,\n",
        "}\n",
        "\n",
        "response_tensor = ppo_trainer.generate([item for item in query_tensor],return_prompt=False, **generation_kwargs)\n",
        "response_txt = tokenizer.decode(response_tensor[0])\n",
        "\n",
        "\n",
        "\n",
        "# 5 Define a reward for response\n",
        "\n",
        "text = [query_txt + response_txt]\n",
        "\n",
        "# Use sentiment analysis pipeline\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)\n",
        "pipe_outputs = sentiment_pipe(text)\n",
        "\n",
        "# Extract sentiment score\n",
        "reward = [torch.tensor(output[\"score\"]) for output in pipe_outputs]\n",
        "\n",
        "\n",
        "rest_ppo = dict()\n",
        "rest_ppo[\"query\"] = query_txt\n",
        "rest_ppo[\"response (RLHF)\"] = response_txt\n",
        "rest_ppo[\"scores (RLHF)\"] = reward\n",
        "\n",
        "\n",
        "\n",
        "# 6 store results in a dataframe\n",
        "df_ppo_results = pd.DataFrame(res\n",
        "                              t_ppo)\n",
        "df_ppo_results\n",
        "\n",
        "df_ppo_results"
      ],
      "metadata": {
        "id": "HAN13EWnl4vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply the method on several querys"
      ],
      "metadata": {
        "id": "zsnq_vJZ0c36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# 1 Load a pretrained model\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
        "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "reward_model = \"lvwerra/distilbert-imdb\"\n",
        "reward_pipe = pipeline(\"sentiment-analysis\", reward_model , device=device)\n",
        "\n",
        "\n",
        "\n",
        "# 2 Initialize trainer\n",
        "\n",
        "ppo_config = {\"batch_size\" : 1}\n",
        "config = PPOConfig(**ppo_config)\n",
        "ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "# 3 Define the query dataset\n",
        "\n",
        "def build_dataset(tokenizer, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
        "    # load imdb with datasets\n",
        "    ds = load_dataset(dataset_name, split=\"train\")\n",
        "    ds = ds.rename_columns({\"text\": \"review\"})\n",
        "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
        "\n",
        "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds\n",
        "\n",
        "\n",
        "dataset = build_dataset(tokenizer)\n",
        "\n",
        "output_min_length = 4\n",
        "output_max_length = 16\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "\n",
        "\n",
        "# 4 get a batch from the dataset\n",
        "\n",
        "bs = 16\n",
        "output_data = dict()\n",
        "dataset.set_format(\"pandas\")\n",
        "df_batch = dataset[:].sample(bs)\n",
        "output_data[\"query\"] = df_batch[\"query\"].tolist()\n",
        "query_tensors = df_batch[\"input_ids\"].tolist()\n",
        "\n",
        "\n",
        "\n",
        "# 5 Generate model response\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\" : -1,\n",
        "    \"top_k\" : 0.0,\n",
        "    \"top_p\" : 1.0,\n",
        "    \"do_sample\" : True,\n",
        "    \"pad_token_id\" : tokenizer.eos_token_id,\n",
        "    \"max_new_tokens\" : 20,\n",
        "}\n",
        "\n",
        "rewards, response_tensors = [], []\n",
        "\n",
        "for query in output_data['query']:\n",
        "  query_txt = query\n",
        "  query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)\n",
        "\n",
        "  response_tensor = ppo_trainer.generate([item for item in query_tensor],return_prompt=False, **generation_kwargs)\n",
        "  response_txt = tokenizer.decode(response_tensor[0])\n",
        "\n",
        "  response_tensors.append(response_txt)\n",
        "\n",
        "  text = [query_txt + response_txt]\n",
        "\n",
        "  # Utilize sentiment analysis pipeline\n",
        "  sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)\n",
        "  pipe_outputs = sentiment_pipe(text)\n",
        "\n",
        "  # Conpute the rewards\n",
        "  reward = [torch.tensor(output[\"score\"]) for output in pipe_outputs]\n",
        "  rewards.append(reward)\n",
        "\n",
        "\n",
        "output_data[\"response (RLHF)\"] = response_tensors\n",
        "output_data[\"scores (RLHF)\"] = rewards\n",
        "\n",
        "\n",
        "\n",
        "# 6 Store results in a dataframe and display them\n",
        "\n",
        "df_ppo_results = pd.DataFrame(output_data)\n",
        "df_ppo_results\n",
        "\n",
        "df_ppo_results"
      ],
      "metadata": {
        "id": "S-jvcHHtNKL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}